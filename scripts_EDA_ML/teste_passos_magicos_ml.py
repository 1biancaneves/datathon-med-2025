# -*- coding: utf-8 -*-
"""Teste_Passos Magicos_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QY40Dx4SRf6zHdwnUCjhSwDy6IEl99a3
"""

# -*- coding: utf-8 -*-
"""
Análise Aprofundada dos Dados da Passos Mágicos

Este script realiza uma análise exploratória dos dados da pesquisa PEDE
e implementa um modelo de Machine Learning para prever o potencial do aluno,
medido pelo Indicador do Ponto de Virada (IPV).
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer







def load_and_prepare_data(files_and_years):
    """
    Essa função é meu "trabalho pesado". Ela vai pegar os arquivos CSV bagunçados,
    juntar tudo e deixar a base de dados brilhando pra análise.
    """
    all_dfs = []
    # Fazer um loop para carregar cada arquivo de uma vez
    for file, year, col_map in files_and_years:
        try:
            df = pd.read_csv(file)
            # Criat uma coluna 'Ano' pra saber de onde veio cada dado depois de juntar tudo
            df['Ano'] = year
            # Padronizar os nomes das colunas, porque cada ano veio com um nome diferente. Assim fica tudo igual.
            df.rename(columns=col_map, inplace=True)
            all_dfs.append(df)
            print(f"Arquivo '{file}' carregado com sucesso para o ano {year}.")
        except FileNotFoundError:
            # Uma segurança pra não quebrar o código se algum arquivo não for encontrado
            print(f"AVISO: O arquivo '{file}' não foi encontrado. Vou pular ele.")
            continue

    if not all_dfs:
        print("ERRO: Nenhum arquivo de dados foi carregado. Encerrando o script.")
        return None

    # Juntar todas as tabelas (2022, 2023, 2024) em uma só, a 'df_combined'
    df_combined = pd.concat(all_dfs, ignore_index=True, sort=False)

    # --- faxina nos dados ---

    # Padronizar la coluna 'Gênero' pra não ter 'masculino' e 'Masculino', por exemplo
    if 'Gênero' in df_combined.columns:
        df_combined['Gênero'] = df_combined['Gênero'].astype(str).str.lower().str.strip().replace({
            'masculino': 'Menino', 'feminino': 'Menina', 'nan': 'Não Informado'
        })

    # Simplificar a coluna 'Instituicao_ensino' para o que realmente importa: se é 'Pública' ou 'Bolsista'
    if 'Instituicao_ensino' in df_combined.columns:
        df_combined['Instituicao_ensino'] = df_combined['Instituicao_ensino'].astype(str).str.lower().str.strip().apply(
            lambda x: 'Pública' if 'pública' in x else ('Bolsista' if 'bolsista' in x or 'decisão' in x else 'Outros')
        )

    # a lista tem todas as colunas que deveriam ser números, mas que vêm como texto com vírgula
    cols_to_convert = ['IAA', 'IEG', 'IPS', 'IDA', 'Nota_Mat', 'Nota_Por', 'Nota_Ing', 'IPV', 'INDE', 'IAN', 'Idade', 'Defasagem', 'IPP', 'Ano ingresso']
    for col in cols_to_convert:
        if col in df_combined.columns:
            # O passo mais IMPORTANTE: troco a vírgula pelo ponto pra poder fazer conta
            df_combined[col] = pd.to_numeric(df_combined[col].astype(str).str.replace(',', '.', regex=False), errors='coerce')

    # --- Engenharia de Features: criando informações novas e valiosas ---

    # Criar a coluna 'Tempo_de_Casa', q vai ser fundamental pra mostrar o impacto da Passos Mágicos ao longo do tempo
    if 'Ano ingresso' in df_combined.columns:
        df_combined['Tempo_de_Casa'] = df_combined['Ano'] - df_combined['Ano ingresso']

    # criar a coluna 'Nivel_Defasagem', q traduz o número da defasagem em categorias fáceis de entender
    if 'Defasagem' in df_combined.columns:
        def categorize_defasagem(d):
            if pd.isna(d): return 'Não Aplicável'
            if d >= 0: return 'Em Dia'
            elif d > -2: return 'Defasagem Leve'
            else: return 'Defasagem Severa'
        df_combined['Nivel_Defasagem'] = df_combined['Defasagem'].apply(categorize_defasagem)

    # Retorno a base de dados final, limpa e pronta para o show!
    return df_combined

def run_exploratory_analysis(df):
    """
    Essa função é a fábrica de gráficos. Pego os dados limpos e gero
    todas as visualizações que vou usar na apresentação.
    """
    if df is None: return
    print("\n--- Gerando os gráficos da Análise Exploratória ---")
    sns.set_style("whitegrid")

    # Gráfico 1: IPV por Defasagem - pra gente ver o impacto da defasagem no potencial
    plt.figure(figsize=(10, 6))
    avg_ipv_defasagem = df.groupby('Nivel_Defasagem')['IPV'].mean().reindex(['Em Dia', 'Defasagem Leve', 'Defasagem Severa'])
    bars = sns.barplot(x=avg_ipv_defasagem.index, y=avg_ipv_defasagem.values, palette=['#28a745', '#ffc107', '#dc3545'])
    plt.title('Potencial (IPV) por Nível de Defasagem', fontsize=18, fontweight='bold')
    plt.ylabel('Média do IPV', fontsize=12)
    plt.xlabel('Nível de Defasagem', fontsize=12)
    for bar in bars.patches:
        if not np.isnan(bar.get_height()):
            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{bar.get_height():.2f}', ha='center', va='bottom', fontsize=11, fontweight='bold')
    plt.savefig('analise_ipv_por_defasagem.png')
    plt.close()

    # Gráfico 2: Correlações com o IPV - o "mapa" do que mais importa pro potencial.!!!
    corr_cols = ['IPV', 'IAA', 'IEG', 'IPS', 'IDA', 'IPP', 'Tempo_de_Casa', 'Defasagem']
    corr_cols = [col for col in corr_cols if col in df.columns]
    correlations = df[corr_cols].corr()['IPV'].drop('IPV').sort_values(ascending=False)
    plt.figure(figsize=(12, 8))
    bars = sns.barplot(x=correlations.values, y=correlations.index, palette="viridis")
    plt.title('Fatores que mais se Correlacionam com o Potencial (IPV)', fontsize=18, fontweight='bold')
    plt.xlabel('Força da Correlação', fontsize=12)
    plt.ylabel('Indicadores', fontsize=12)
    for i, v in enumerate(correlations.values):
        plt.text(v + 0.01 if v > 0 else v - 0.05, i, f'{v:.2f}', va='center', fontweight='bold')
    plt.savefig('analise_correlacoes_ipv.png')
    plt.close()

    # Gráfico 3: IPV por Tempo de Casa - a prova do impacto da Passos Mágicos!!!!
    plt.figure(figsize=(12, 7))
    avg_ipv_tempo = df.groupby('Tempo_de_Casa')['IPV'].mean()
    sns.lineplot(x=avg_ipv_tempo.index, y=avg_ipv_tempo.values, marker='o', color='purple', linewidth=3)
    plt.title('Evolução do Potencial (IPV) ao Longo dos Anos na Associação', fontsize=18, fontweight='bold')
    plt.xlabel('Anos na Passos Mágicos', fontsize=12)
    plt.ylabel('Média do IPV', fontsize=12)
    if not avg_ipv_tempo.empty:
      plt.xticks(np.arange(int(min(avg_ipv_tempo.index)), int(max(avg_ipv_tempo.index))+1, 1.0))
    for i, txt in enumerate(avg_ipv_tempo.values):
        if not pd.isna(txt):
            plt.text(avg_ipv_tempo.index[i], txt + 0.05, f'{txt:.2f}', ha='center', fontsize=11, fontweight='bold')
    plt.savefig('analise_ipv_por_tempo_casa.png')
    plt.close()

    print("Gráficos da Análise Exploratória foram salvos com sucesso!")

# --- Parte 3: Modelo de Machine Learning ---

def build_and_evaluate_model(df):
    """
    Aqui a mágica acontece. Vou usar os dados pra treinar um modelo que
    tenta prever se um aluno vai atingir o "Ponto de Virada".
    """
    if df is None: return
    print("\n--- Construindo o Modelo de Machine Learning ---")

    # Crio minha variável alvo: um 'sim' ou 'não' (1 ou 0) pra quem atingiu o Ponto de Virada.
    # Com base nos relatórios, defini o corte em 8.5.
    df['Atingiu_PV'] = (df['IPV'] >= 8.5).astype(int)

    # Essas são as pistas que vou dar pro meu modelo aprender a prever o resultado.
    features = ['Idade', 'Tempo_de_Casa', 'Defasagem', 'IDA', 'IEG', 'IPS', 'IPP', 'IAA', 'Gênero', 'Instituicao_ensino']
    target = 'Atingiu_PV'

    for col in features:
        if col not in df.columns: df[col] = np.nan

    df_model = df.copy()

    if len(df_model) < 50:
        print("AVISO: Dados insuficientes para treinar um modelo.")
        return

    # Separo meus dados: um pedaço pra treinar o modelo e outro pra testar se ele aprendeu direito, sem colar
    X = df_model[features]
    y = df_model[target]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

    # Pipeline de pré-processamento: um jeito organizado de preparar os dados para o modelo
    # Ele trata as colunas de número e de texto automaticamente
    numeric_features = ['Idade', 'Tempo_de_Casa', 'Defasagem', 'IDA', 'IEG', 'IPS', 'IPP', 'IAA']
    categorical_features = ['Gênero', 'Instituicao_ensino']
    numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])
    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore'))])
    preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features), ('cat', categorical_transformer, categorical_features)])

    # Escolhi a Regressão Logística porque ela é boa pra prever 'sim' ou 'não' e me deixa ver o que foi mais importante pra decisão dela
    model = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LogisticRegression(random_state=42, class_weight='balanced'))])

    # Mando o modelo 'estudar' os dados de treino
    print("Treinando o modelo...")
    model.fit(X_train, y_train)

    # Peço um 'boletim' do modelo pra ver a performance del.
    print("Avaliando o modelo...")
    y_pred = model.predict(X_test)
    print("\nRelatório de Classificação:\n", classification_report(y_test, y_pred))

    # gera a Matriz de Confusão pra ver onde o modelo acertou e errou
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Não Atingiu', 'Atingiu'], yticklabels=['Não Atingiu', 'Atingiu'])
    plt.title('Matriz de Confusão', fontsize=16)
    plt.ylabel('Real', fontsize=12)
    plt.xlabel('Previsto', fontsize=12)
    plt.savefig('analise_matriz_confusao.png')
    plt.close()

    # pega os 'pesos' que o modelo deu pra cada fator e faço um gráfico
    try:
        coefficients = model.named_steps['classifier'].coef_[0]
        feature_names_raw = numeric_features + list(model.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_features))
        feature_importance = pd.DataFrame({'Feature': feature_names_raw, 'Importance': np.abs(coefficients)}).sort_values('Importance', ascending=True).tail(10)

        plt.figure(figsize=(12, 8))
        feature_importance.plot(x='Feature', y='Importance', kind='barh', figsize=(12, 8), color='skyblue')
        plt.title('Principais Fatores para Atingir o Ponto de Virada', fontsize=18, fontweight='bold')
        plt.xlabel('Importância (Coeficiente Absoluto)', fontsize=12)
        plt.ylabel('Fator', fontsize=12)
        plt.tight_layout()
        plt.savefig('analise_importancia_features.png')
        plt.close()
        print("\nO gráfico 'Principais Fatores' mostra o que o modelo considerou mais importante pra prever o sucesso.")
    except Exception as e:
        print(f"Não foi possível gerar o gráfico de importância das features: {e}")
    print("Modelo de Machine Learning Concluído")



# --- Execução Principal ---

if __name__ == "__main__":
    # Aqui eu configuro quais arquivos quero carregar e como quero renomear as colunas de cada um
    files_to_load = [
        ('PEDE2024-HACKATHONMED2025 - PEDE2022.csv', 2022, {'Idade 22': 'Idade', 'Gênero': 'Gênero', 'Instituição de ensino': 'Instituicao_ensino', 'INDE 22': 'INDE', 'Matem': 'Nota_Mat', 'Portug': 'Nota_Por', 'Inglês': 'Nota_Ing', 'Defas': 'Defasagem', 'Ano Ingresso': 'Ano ingresso'}),
        ('PEDE2024-HACKATHONMED2025 - PEDE2023.csv', 2023, {'Gênero': 'Gênero', 'Instituição de ensino': 'Instituicao_ensino', 'INDE 23': 'INDE', 'Mat': 'Nota_Mat', 'Por': 'Nota_Por', 'Ing': 'Nota_Ing', 'Ano de ingresso': 'Ano ingresso'}),
        ('PEDE2024-HACKATHONMED2025 - PEDE2024.csv', 2024, {'Gênero': 'Gênero', 'Instituição de ensino': 'Instituicao_ensino', 'INDE 2024': 'INDE', 'Mat': 'Nota_Mat', 'Por': 'Nota_Por', 'Ing': 'Nota_Ing', 'Ano de ingresso': 'Ano ingresso'})
    ]

    # 1. Chamo a função pra fazer toda a limpeza e me devolver a base de dados pronta
    df_completo = load_and_prepare_data(files_to_load)

    if df_completo is not None:
        # 2. Rodo a análise exploratória pra gerar os gráficos
        run_exploratory_analysis(df_completo)

        # 3. Construo e avalio meu modelo de machine learning
        build_and_evaluate_model(df_completo)

        # 4. A cereja do bolo: salvo o dataset final, já tratado, em um novo arquivo CSV
        # Coloquei o separador como ';' e o decimal como ',' pra abrir certinho no Excel
        output_filename = 'dataset_tratado_passosmagicos.csv'
        df_completo.to_csv(output_filename, index=False, decimal=',', sep=';')
        print(f"\nPronto! Dataset tratado e unificado foi salvo como '{output_filename}'")